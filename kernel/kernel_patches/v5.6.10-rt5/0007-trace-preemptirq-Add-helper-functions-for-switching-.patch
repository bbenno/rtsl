From 58043d6d6afde51f894081b93b751dd1562be06b Mon Sep 17 00:00:00 2001
Message-Id: <58043d6d6afde51f894081b93b751dd1562be06b.1589812797.git.bristot@redhat.com>
In-Reply-To: <71e1b6cffa772a5839c4f175db4ad52e1fbec89d.1589812797.git.bristot@redhat.com>
References: <71e1b6cffa772a5839c4f175db4ad52e1fbec89d.1589812797.git.bristot@redhat.com>
From: Daniel Bristot de Oliveira <bristot@redhat.com>
Date: Wed, 22 Apr 2020 11:24:58 +0200
Subject: [PATCH 07/16] trace/preemptirq: Add helper functions for switching to
 the scheduling context

For optimization reasons, if the preemption was already disabled to
postpone the scheduler, it will not be re enabled and disabled back to
call the scheduler. It makes sense.

So, we need to add an annotation for this context. These helper
functions do that.

No functional change, and no code add if the preempt tracing is off.

Signed-off-by: Daniel Bristot de Oliveira <bristot@redhat.com>
---
 include/linux/ftrace.h          |  4 ++++
 kernel/trace/trace_preemptirq.c | 24 ++++++++++++++++++++++++
 2 files changed, 28 insertions(+)

diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 7345227ca749..dfa5d0f55a3c 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -804,6 +804,8 @@ static inline unsigned long get_lock_parent_ip(void)
 #ifdef CONFIG_TRACE_PREEMPT_TOGGLE
   extern void trace_preempt_on(unsigned long a0, unsigned long a1, int to_sched);
   extern void trace_preempt_off(unsigned long a0, unsigned long a1, int to_sched);
+  extern void trace_preempt_switch_to_sched(unsigned long a0, unsigned long a1);
+  extern void trace_preempt_switch_not_sched(unsigned long a0, unsigned long a1);
 #else
 /*
  * Use defines instead of static inlines because some arches will make code out
@@ -811,6 +813,8 @@ static inline unsigned long get_lock_parent_ip(void)
  */
 # define trace_preempt_on(a0, a1, to_sched) do { } while (0)
 # define trace_preempt_off(a0, a1, to_sched) do { } while (0)
+# define trace_preempt_switch_to_sched(a0, a1) do { } while (0)
+# define trace_preempt_switch_not_sched(a0, a1) do { } while (0)
 #endif
 
 #ifdef CONFIG_FTRACE_MCOUNT_RECORD
diff --git a/kernel/trace/trace_preemptirq.c b/kernel/trace/trace_preemptirq.c
index 5253f7d2d2b2..ed5fcaaef1e1 100644
--- a/kernel/trace/trace_preemptirq.c
+++ b/kernel/trace/trace_preemptirq.c
@@ -91,4 +91,28 @@ void trace_preempt_off(unsigned long a0, unsigned long a1, int to_sched)
 		trace_preempt_disable_rcuidle(a0, a1, to_sched);
 	tracer_preempt_off(a0, a1);
 }
+
+void trace_preempt_switch_to_sched(unsigned long a0, unsigned long a1)
+{
+	/*
+	 * We are not actually changing the preempt counter, just changing
+	 * the context in which the preemption was disabled.
+	 *
+	 * here: from the preempt_disable() -> preempt_disable_sched().
+	 */
+	trace_preempt_on(a0, a1, 0);
+	trace_preempt_off(a0, a1, 1);
+}
+
+void trace_preempt_switch_not_sched(unsigned long a0, unsigned long a1)
+{
+	/*
+	 * We are not actually changing the preempt counter, just changing
+	 * the context in which the preemption was disabled.
+	 *
+	 * here: from the preempt_disable_sched() -> preempt_disable().
+	 */
+	trace_preempt_on(a0, a1, 1);
+	trace_preempt_off(a0, a1, 0);
+}
 #endif
-- 
2.26.2

